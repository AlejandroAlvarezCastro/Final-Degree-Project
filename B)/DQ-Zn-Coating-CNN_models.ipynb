{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9a731d",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dea4791-4b1e-48fa-9e6d-c76a4e43df23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import sys, os, datetime, pickle, time\n",
    "import string, pdb, tqdm\n",
    "import random, keras, os.path, yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "#\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime, timedelta, date\n",
    "from PIL import Image\n",
    "from IPython.display import Image\n",
    "# %matplotlib inline\n",
    "\n",
    "#\n",
    "tf.autograph.set_verbosity(0)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "# config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4511fde",
   "metadata": {},
   "source": [
    "# 2. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9400b793-9e42-419a-a2fd-695832b87bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta del directorio tres niveles arriba\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir))\n",
    "\n",
    "# Cargar datasets desde el directorio tres niveles arriba\n",
    "data_norm_path = os.path.join(parent_dir, 'Dataset/data_norm.pkl')\n",
    "\n",
    "with open(data_norm_path, 'rb') as handle:\n",
    "    GlblFrm = pickle.load(handle)\n",
    "    dosD    = pickle.load(handle)\n",
    "    coils   = pickle.load(handle)\n",
    "    nclmaps = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9e127",
   "metadata": {},
   "source": [
    "# 3. Building Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39aee1b5-9db4-462a-9e19-22c2972c9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Selecting coils OK and NOK\n",
    "lsids1 = coils.loc[coils['Label']==1,'SID'].tolist()\n",
    "lsids2 = coils.loc[coils['Label']==2,'SID'].tolist()\n",
    "#\n",
    "# Select and extract 30 coil's sids for independent assessemnt\n",
    "ass1 = random.sample(lsids1, 30)\n",
    "ass2 = random.sample(lsids2, 30)\n",
    "res1 = np.array(list(set(lsids1) - set(ass1)))\n",
    "res2 = np.array(list(set(lsids2) - set(ass2)))\n",
    "#\n",
    "# The remaining are organized for building the model\n",
    "random.shuffle(res1)\n",
    "random.shuffle(res2)\n",
    "train1, validate1, test1 = np.split(res1,[int(.7*len(res1)), int(.9*len(res1))])\n",
    "train2, validate2, test2 = np.split(res2,[int(.7*len(res2)), int(.9*len(res2))])\n",
    "#\n",
    "# Building the full sets\n",
    "train = train1.tolist() + train2.tolist()\n",
    "valid = validate1.tolist() + validate2.tolist()\n",
    "test  = test1.tolist()  + test2.tolist()\n",
    "ass   = ass1   + ass2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82306ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMap(id, nlcmaps):\n",
    "    arr1 = nclmaps[id][1234]['nzne'].to_numpy()\n",
    "    arr2 = nclmaps[id][1243]['nzne'].to_numpy()\n",
    "    arr  = np.concatenate((arr1, arr2), axis=1)\n",
    "    return(arr)\n",
    "#\n",
    "def prep_dataset(setd,nlcmaps):\n",
    "    setd_f = []\n",
    "    setd_l = []\n",
    "    for i in setd:\n",
    "        arrimg = featureMap(i, nclmaps)\n",
    "        lbl = coils.loc[coils['SID']==i,'Label'].values[0]\n",
    "        setd_f.append(arrimg)\n",
    "        if lbl == 1:\n",
    "            setd_l.append([1.,0.])\n",
    "        if lbl == 2:\n",
    "            setd_l.append([0.,1.])\n",
    "    setd_f = np.array(setd_f)\n",
    "    setd_l = np.array(setd_l)\n",
    "    return([setd_f,setd_l])\n",
    "#\n",
    "def factory_rep(arr,step=0):\n",
    "    # arr image havind 264 rows and 18 (9 face A+9 face B) columns of normalized data \n",
    "    # mirror per face over x axis\n",
    "    # channels 0-3 => 5-8 and 5-8 => 0-3\n",
    "    permut1 = list(range(8,4,-1))+[4] + list(range(3,-1,-1))+ list(range(17,13,-1))+[13]+list(range(12,8,-1))\n",
    "    idx = np.empty_like(permut1)\n",
    "    idx[permut1] = np.arange(len(permut1))\n",
    "    arr1 = arr[:,idx]\n",
    "    permut2 = list(range(263,131,-1))+ list(range(131,-1,-1))\n",
    "    idx = np.empty_like(permut2)\n",
    "    idx[permut2] = np.arange(len(permut2))    \n",
    "    arr2 = arr[idx,:]\n",
    "    arr3 = arr1[idx,:]\n",
    "    res  = [arr, arr1, arr2, arr3]\n",
    "    if step > 0:\n",
    "        newa = arr\n",
    "        end = arr.shape[0]-step\n",
    "        for i in range(arr.shape[0] // step):\n",
    "            permut = list(range(arr.shape[0]-step,arr.shape[0]))+list(range(0,end))\n",
    "            idx = np.empty_like(permut)\n",
    "            idx[permut] = np.arange(len(permut))\n",
    "            newb= newa[idx,:]\n",
    "            res.append(newb)\n",
    "            newa= newb\n",
    "    res = np.array(res)\n",
    "    return(res)\n",
    "#\n",
    "\n",
    "def prep_dataset_aug(setd,nlcmaps,tlab=-1):\n",
    "    setd_f = []\n",
    "    setd_l = []\n",
    "    for i in setd:\n",
    "        arrimg = featureMap(i, nclmaps)\n",
    "        lbl = coils.loc[coils['SID']==i,'Label'].values[0]\n",
    "        if lbl == tlab: # if lower class => higher augmentation\n",
    "            res = factory_rep(arrimg,step=8)\n",
    "        else:\n",
    "            res = factory_rep(arrimg)\n",
    "        for j in range(res.shape[0]):\n",
    "            setd_f.append(res[j,:,:])\n",
    "            if lbl == 1:\n",
    "                setd_l.append([1.,0.])\n",
    "            if lbl == 2:\n",
    "                setd_l.append([0.,1.])\n",
    "    setd_f = np.array(setd_f)\n",
    "    setd_l = np.array(setd_l)\n",
    "    return([setd_f,setd_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64affff8-d837-4739-828a-b62b3d3158f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 'C:/Users/alexm/OneDrive/Escritorio/UNIVERSIDAD/TFG/ALVAREZ_CASTRO_Alejandro/ALVAREZ_CASTRO_Alejandro/ZN_1D_imgs'\n",
    "# for i in train + valid + test:\n",
    "train_f, train_l = prep_dataset_aug(train,nclmaps,2)\n",
    "np.savez(goal+'train.npz', features=train_f, labels=train_l)\n",
    "#\n",
    "valid_f, valid_l = prep_dataset(valid,nclmaps)\n",
    "np.savez(goal+'validation.npz', features=valid_f, labels=valid_l)\n",
    "#\n",
    "test_f, test_l = prep_dataset(test,nclmaps)\n",
    "np.savez(goal+'test.npz', features=test_f, labels=test_l)\n",
    "#\n",
    "ass_f, ass_l = prep_dataset(ass,nclmaps)\n",
    "np.savez(goal+'assess.npz', features=ass_f, labels=ass_l)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1b67e1-3a10-426a-8c8b-bac30e6340f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4293, 264, 18)\n",
      "(4293, 2)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "npzfile = np.load(goal+'train.npz')\n",
    "print(npzfile['features'].shape)\n",
    "print(npzfile['labels'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ec863",
   "metadata": {},
   "source": [
    "# 4. Building and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5979cd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.lib.npyio.NpzFile at 0x1db0a954a30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ruta donde se encuentran los datos y donde se guardarán los archivos de anotación\n",
    "data_folder = 'C:/Users/alexm/OneDrive/Escritorio/UNIVERSIDAD/TFG/ALVAREZ_CASTRO_Alejandro/ALVAREZ_CASTRO_Alejandro/ZN_1D_imgs'\n",
    "annotation_folder = 'C:/Users/alexm/OneDrive/Escritorio/UNIVERSIDAD/TFG/ALVAREZ_CASTRO_Alejandro/ALVAREZ_CASTRO_Alejandro/anotation'\n",
    "\n",
    "# Carga de los archivos .npz que contienen los datos de entrenamiento, validación, prueba y evaluación\n",
    "train_data = np.load(data_folder + \"train.npz\")\n",
    "valid_data = np.load(data_folder + \"validation.npz\")\n",
    "test_data = np.load(data_folder + \"test.npz\")\n",
    "assess_data = np.load(data_folder + \"assess.npz\")\n",
    "\n",
    "assess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9638d577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves de asses_data:  ['features', 'labels']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NpzFile' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClaves de asses_data: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlist\u001b[39m(assess_data\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensiones de assess_data : \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43massess_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensiones de features: \u001b[39m\u001b[38;5;124m\"\u001b[39m,assess_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensiones de labels: \u001b[39m\u001b[38;5;124m\"\u001b[39m,assess_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NpzFile' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    "print(\"Claves de asses_data: \",list(assess_data.keys()))\n",
    "print(\"Dimensiones de features: \",assess_data['features'].shape)\n",
    "print(\"Dimensiones de labels: \",assess_data['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para escribir la información de anotación en archivos de texto\n",
    "def write_annotation_file(data, annotation_folder, prefix):\n",
    "    annotations = data[\"labels\"]\n",
    "    images = data[\"features\"]\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        annotation = annotations[i]\n",
    "        \n",
    "        # Obtener las coordenadas de los cuadros delimitadores y la clase\n",
    "        # Supongamos que las coordenadas se encuentran en las primeras 4 columnas del array\n",
    "        bbox_coordinates = annotation[:4]\n",
    "        label = annotation[4]  # 1 para \"OK\", 2 para \"NOK\"\n",
    "        \n",
    "        # Escribir las coordenadas y la clase en el archivo de anotación\n",
    "        annotation_file_path = os.path.join(annotation_folder, f\"{prefix}_{i}.txt\")\n",
    "        with open(annotation_file_path, \"w\") as f:\n",
    "            f.write(f\"{label} {' '.join(map(str, bbox_coordinates))}\")\n",
    "        print(f\"Archivo de anotación guardado en {annotation_file_path}\")\n",
    "\n",
    "# Escribir archivos de anotación para los conjuntos de entrenamiento, validación, prueba y evaluación\n",
    "write_annotation_file(train_data, annotation_folder, \"train\")\n",
    "write_annotation_file(valid_data, annotation_folder, \"validation\")\n",
    "write_annotation_file(test_data, annotation_folder, \"test\")\n",
    "write_annotation_file(assess_data, annotation_folder, \"assess\")\n",
    "\n",
    "# Crear archivos de configuración (ejemplo)\n",
    "# Esto puede variar dependiendo de los detalles específicos de tu configuración y entrenamiento de YOLOv7\n",
    "config_template = \"\"\"\n",
    "[dataset]\n",
    "train = ruta/a/tus/datos/train.txt\n",
    "val = ruta/a/tus/datos/validation.txt\n",
    "test = ruta/a/tus/datos/test.txt\n",
    "names = ruta/a/tus/datos/classes.names\n",
    "backup = backup/\n",
    "\n",
    "[net]\n",
    "# Tú configuración de red\n",
    "\n",
    "[training]\n",
    "# Tú configuración de entrenamiento\n",
    "\"\"\"\n",
    "config_file_path = \"ruta/a/tus/datos/yolov7_custom_config.cfg\"\n",
    "with open(config_file_path, \"w\") as f:\n",
    "    f.write(config_template)\n",
    "print(f\"Archivo de configuración guardado en {config_file_path}\")\n",
    "\n",
    "# Organizar la estructura de carpetas\n",
    "os.makedirs(os.path.join(data_folder, \"images\"))\n",
    "os.makedirs(os.path.join(data_folder, \"labels\"))\n",
    "os.rename(os.path.join(annotation_folder, \"*.txt\"), os.path.join(data_folder, \"labels\"))\n",
    "\n",
    "print(\"Estructura de carpetas organizada correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
