{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9a731d",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dea4791-4b1e-48fa-9e6d-c76a4e43df23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import sys, os, datetime, pickle, time\n",
    "import string, pdb, tqdm\n",
    "import random, keras, os.path, yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime, timedelta, date\n",
    "from PIL import Image\n",
    "from IPython.display import Image\n",
    "#\n",
    "import yolov7   # https://pypi.org/project/yolov7-easy/\n",
    "# %matplotlib inline\n",
    "\n",
    "#\n",
    "tf.autograph.set_verbosity(0)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "# config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4511fde",
   "metadata": {},
   "source": [
    "# 2. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9400b793-9e42-419a-a2fd-695832b87bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta del directorio tres niveles arriba\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir))\n",
    "\n",
    "# Cargar datasets desde el directorio tres niveles arriba\n",
    "data_norm_path = os.path.join(parent_dir, 'Dataset/data_norm.pkl')\n",
    "\n",
    "with open(data_norm_path, 'rb') as handle:\n",
    "    GlblFrm = pickle.load(handle)\n",
    "    dosD    = pickle.load(handle)\n",
    "    coils   = pickle.load(handle)\n",
    "    nclmaps = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9e127",
   "metadata": {},
   "source": [
    "# 3. Building Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39aee1b5-9db4-462a-9e19-22c2972c9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Selecting coils OK and NOK\n",
    "lsids1 = coils.loc[coils['Label']==1,'SID'].tolist()\n",
    "lsids2 = coils.loc[coils['Label']==2,'SID'].tolist()\n",
    "#\n",
    "# Select and extract 30 coil's sids for independent assessemnt\n",
    "ass1 = random.sample(lsids1, 30)\n",
    "ass2 = random.sample(lsids2, 30)\n",
    "res1 = np.array(list(set(lsids1) - set(ass1)))\n",
    "res2 = np.array(list(set(lsids2) - set(ass2)))\n",
    "#\n",
    "# The remaining are organized for building the model\n",
    "random.shuffle(res1)\n",
    "random.shuffle(res2)\n",
    "train1, validate1, test1 = np.split(res1,[int(.7*len(res1)), int(.9*len(res1))])\n",
    "train2, validate2, test2 = np.split(res2,[int(.7*len(res2)), int(.9*len(res2))])\n",
    "#\n",
    "# Building the full sets\n",
    "train = train1.tolist() + train2.tolist()\n",
    "valid = validate1.tolist() + validate2.tolist()\n",
    "test  = test1.tolist()  + test2.tolist()\n",
    "ass   = ass1   + ass2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82306ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMap(id, nlcmaps):\n",
    "    arr1 = nclmaps[id][1234]['nzne'].to_numpy()\n",
    "    arr2 = nclmaps[id][1243]['nzne'].to_numpy()\n",
    "    arr  = np.concatenate((arr1, arr2), axis=1)\n",
    "    return(arr)\n",
    "#\n",
    "def prep_dataset(setd,nlcmaps):\n",
    "    setd_f = []\n",
    "    setd_l = []\n",
    "    for i in setd:\n",
    "        arrimg = featureMap(i, nclmaps)\n",
    "        lbl = coils.loc[coils['SID']==i,'Label'].values[0]\n",
    "        setd_f.append(arrimg)\n",
    "        if lbl == 1:\n",
    "            setd_l.append([1.,0.])\n",
    "        if lbl == 2:\n",
    "            setd_l.append([0.,1.])\n",
    "    setd_f = np.array(setd_f)\n",
    "    setd_l = np.array(setd_l)\n",
    "    return([setd_f,setd_l])\n",
    "#\n",
    "def factory_rep(arr,step=0):\n",
    "    # arr image havind 264 rows and 18 (9 face A+9 face B) columns of normalized data \n",
    "    # mirror per face over x axis\n",
    "    # channels 0-3 => 5-8 and 5-8 => 0-3\n",
    "    permut1 = list(range(8,4,-1))+[4] + list(range(3,-1,-1))+ list(range(17,13,-1))+[13]+list(range(12,8,-1))\n",
    "    idx = np.empty_like(permut1)\n",
    "    idx[permut1] = np.arange(len(permut1))\n",
    "    arr1 = arr[:,idx]\n",
    "    permut2 = list(range(263,131,-1))+ list(range(131,-1,-1))\n",
    "    idx = np.empty_like(permut2)\n",
    "    idx[permut2] = np.arange(len(permut2))    \n",
    "    arr2 = arr[idx,:]\n",
    "    arr3 = arr1[idx,:]\n",
    "    res  = [arr, arr1, arr2, arr3]\n",
    "    if step > 0:\n",
    "        newa = arr\n",
    "        end = arr.shape[0]-step\n",
    "        for i in range(arr.shape[0] // step):\n",
    "            permut = list(range(arr.shape[0]-step,arr.shape[0]))+list(range(0,end))\n",
    "            idx = np.empty_like(permut)\n",
    "            idx[permut] = np.arange(len(permut))\n",
    "            newb= newa[idx,:]\n",
    "            res.append(newb)\n",
    "            newa= newb\n",
    "    res = np.array(res)\n",
    "    return(res)\n",
    "#\n",
    "\n",
    "def prep_dataset_aug(setd,nlcmaps,tlab=-1):\n",
    "    setd_f = []\n",
    "    setd_l = []\n",
    "    for i in setd:\n",
    "        arrimg = featureMap(i, nclmaps)\n",
    "        lbl = coils.loc[coils['SID']==i,'Label'].values[0]\n",
    "        if lbl == tlab: # if lower class => higher augmentation\n",
    "            res = factory_rep(arrimg,step=8)\n",
    "        else:\n",
    "            res = factory_rep(arrimg)\n",
    "        for j in range(res.shape[0]):\n",
    "            setd_f.append(res[j,:,:])\n",
    "            if lbl == 1:\n",
    "                setd_l.append([1.,0.])\n",
    "            if lbl == 2:\n",
    "                setd_l.append([0.,1.])\n",
    "    setd_f = np.array(setd_f)\n",
    "    setd_l = np.array(setd_l)\n",
    "    return([setd_f,setd_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64affff8-d837-4739-828a-b62b3d3158f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 'C:/Users/alexm/OneDrive/Escritorio/UNIVERSIDAD/TFG/ALVAREZ_CASTRO_Alejandro/ALVAREZ_CASTRO_Alejandro/ZN_1D_imgs'\n",
    "# for i in train + valid + test:\n",
    "train_f, train_l = prep_dataset_aug(train,nclmaps,2)\n",
    "np.savez(goal+'train.npz', features=train_f, labels=train_l)\n",
    "#\n",
    "valid_f, valid_l = prep_dataset(valid,nclmaps)\n",
    "np.savez(goal+'validation.npz', features=valid_f, labels=valid_l)\n",
    "#\n",
    "test_f, test_l = prep_dataset(test,nclmaps)\n",
    "np.savez(goal+'test.npz', features=test_f, labels=test_l)\n",
    "#\n",
    "ass_f, ass_l = prep_dataset(ass,nclmaps)\n",
    "np.savez(goal+'assess.npz', features=ass_f, labels=ass_l)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1b67e1-3a10-426a-8c8b-bac30e6340f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4293, 264, 18)\n",
      "(4293, 2)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "npzfile = np.load(goal+'train.npz')\n",
    "print(npzfile['features'].shape)\n",
    "print(npzfile['labels'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ec863",
   "metadata": {},
   "source": [
    "# 4. Building and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5979cd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.lib.npyio.NpzFile at 0x18ff5fcc310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ruta donde se encuentran los datos y donde se guardar치n los archivos de anotaci칩n\n",
    "data_folder = 'C:/Users/alexm/OneDrive/Escritorio/UNIVERSIDAD/TFG/ALVAREZ_CASTRO_Alejandro/ALVAREZ_CASTRO_Alejandro/ZN_1D_imgs'\n",
    "annotation_folder = 'C:/Users/alexm/OneDrive/Escritorio/UNIVERSIDAD/TFG/ALVAREZ_CASTRO_Alejandro/ALVAREZ_CASTRO_Alejandro/anotation'\n",
    "\n",
    "# Carga de los archivos .npz que contienen los datos de entrenamiento, validaci칩n, prueba y evaluaci칩n\n",
    "train_data = np.load(data_folder + \"train.npz\")\n",
    "valid_data = np.load(data_folder + \"validation.npz\")\n",
    "test_data = np.load(data_folder + \"test.npz\")\n",
    "assess_data = np.load(data_folder + \"assess.npz\")\n",
    "\n",
    "assess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9638d577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves de asses_data:  ['features', 'labels']\n",
      "Dimensiones de features:  (60, 264, 18)\n",
      "Dimensiones de labels:  (60, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Claves de asses_data: \",list(assess_data.keys()))\n",
    "print(\"Dimensiones de features: \",assess_data['features'].shape)\n",
    "print(\"Dimensiones de labels: \",assess_data['labels'].shape)\n",
    "\n",
    "assess_data['features'].ndim\n",
    "assess_data['labels'].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d3ab4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo pre-entrenado YOLOv7 desde el archivo .pt\n",
    "model = yolov7.load('yolov7.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1644984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "model.conf = 0.25  # NMS confidence threshold\n",
    "model.iou = 0.45  # NMS IoU threshold\n",
    "model.classes = None  # (optional list) filter by class\n",
    "\n",
    "# set image\n",
    "imgs = train_data['features']\n",
    "\n",
    "# perform inference\n",
    "results = model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68d64d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with larger input size and test time augmentation\n",
    "results = model(imgs, size=1280, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1081685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yolov7.models.common.Detections at 0x19017eff2b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656173fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse results\n",
    "predictions = results.pred[0]\n",
    "boxes = predictions[:, :4] # x1, y1, x2, y2\n",
    "scores = predictions[:, 4]\n",
    "categories = predictions[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "241e1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show detection bounding boxes on image\n",
    "results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
