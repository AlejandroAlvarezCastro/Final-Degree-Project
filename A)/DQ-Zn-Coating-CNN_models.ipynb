{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9a731d",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea4791-4b1e-48fa-9e6d-c76a4e43df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, datetime, pickle, time\n",
    "import string, pdb, tqdm\n",
    "import random, keras, os.path, yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "from functions import *\n",
    "from models import *\n",
    "from Builder import ConvNetBuilder\n",
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "#\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime, timedelta, date\n",
    "from PIL import Image\n",
    "from IPython.display import Image\n",
    "# %matplotlib inline\n",
    "\n",
    "#\n",
    "tf.autograph.set_verbosity(0)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "# config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4511fde",
   "metadata": {},
   "source": [
    "# 2. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400b793-9e42-419a-a2fd-695832b87bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta del directorio tres niveles arriba\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir))\n",
    "\n",
    "# Cargar datasets desde el directorio tres niveles arriba\n",
    "data_norm_path = os.path.join(parent_dir, 'Dataset/data_norm.pkl')\n",
    "\n",
    "with open(data_norm_path, 'rb') as handle:\n",
    "    GlblFrm = pickle.load(handle)\n",
    "    dosD    = pickle.load(handle)\n",
    "    coils   = pickle.load(handle)\n",
    "    nclmaps = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9e127",
   "metadata": {},
   "source": [
    "# 3. Building Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aee1b5-9db4-462a-9e19-22c2972c9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Selecting coils OK and NOK\n",
    "lsids1 = coils.loc[coils['Label']==1,'SID'].tolist()\n",
    "lsids2 = coils.loc[coils['Label']==2,'SID'].tolist()\n",
    "#\n",
    "# Select and extract 30 coil's sids for independent assessemnt\n",
    "ass1 = random.sample(lsids1, 30)\n",
    "ass2 = random.sample(lsids2, 30)\n",
    "res1 = np.array(list(set(lsids1) - set(ass1)))\n",
    "res2 = np.array(list(set(lsids2) - set(ass2)))\n",
    "#\n",
    "# The remaining are organized for building the model\n",
    "random.shuffle(res1)\n",
    "random.shuffle(res2)\n",
    "train1, validate1, test1 = np.split(res1,[int(.7*len(res1)), int(.9*len(res1))])\n",
    "train2, validate2, test2 = np.split(res2,[int(.7*len(res2)), int(.9*len(res2))])\n",
    "#\n",
    "# Building the full sets\n",
    "train = train1.tolist() + train2.tolist()\n",
    "valid = validate1.tolist() + validate2.tolist()\n",
    "test  = test1.tolist()  + test2.tolist()\n",
    "ass   = ass1   + ass2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82306ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMap(id, nlcmaps):\n",
    "    arr1 = nclmaps[id][1234]['nzne'].to_numpy()\n",
    "    arr2 = nclmaps[id][1243]['nzne'].to_numpy()\n",
    "    arr  = np.concatenate((arr1, arr2), axis=1)\n",
    "    return(arr)\n",
    "#\n",
    "def prep_dataset(setd,nlcmaps):\n",
    "    setd_f = []\n",
    "    setd_l = []\n",
    "    for i in setd:\n",
    "        arrimg = featureMap(i, nclmaps)\n",
    "        lbl = coils.loc[coils['SID']==i,'Label'].values[0]\n",
    "        setd_f.append(arrimg)\n",
    "        if lbl == 1:\n",
    "            setd_l.append([1.,0.])\n",
    "        if lbl == 2:\n",
    "            setd_l.append([0.,1.])\n",
    "    setd_f = np.array(setd_f)\n",
    "    setd_l = np.array(setd_l)\n",
    "    return([setd_f,setd_l])\n",
    "#\n",
    "def factory_rep(arr,step=0):\n",
    "    # arr image havind 264 rows and 18 (9 face A+9 face B) columns of normalized data \n",
    "    # mirror per face over x axis\n",
    "    # channels 0-3 => 5-8 and 5-8 => 0-3\n",
    "    permut1 = list(range(8,4,-1))+[4] + list(range(3,-1,-1))+ list(range(17,13,-1))+[13]+list(range(12,8,-1))\n",
    "    idx = np.empty_like(permut1)\n",
    "    idx[permut1] = np.arange(len(permut1))\n",
    "    arr1 = arr[:,idx]\n",
    "    permut2 = list(range(263,131,-1))+ list(range(131,-1,-1))\n",
    "    idx = np.empty_like(permut2)\n",
    "    idx[permut2] = np.arange(len(permut2))    \n",
    "    arr2 = arr[idx,:]\n",
    "    arr3 = arr1[idx,:]\n",
    "    res  = [arr, arr1, arr2, arr3]\n",
    "    if step > 0:\n",
    "        newa = arr\n",
    "        end = arr.shape[0]-step\n",
    "        for i in range(arr.shape[0] // step):\n",
    "            permut = list(range(arr.shape[0]-step,arr.shape[0]))+list(range(0,end))\n",
    "            idx = np.empty_like(permut)\n",
    "            idx[permut] = np.arange(len(permut))\n",
    "            newb= newa[idx,:]\n",
    "            res.append(newb)\n",
    "            newa= newb\n",
    "    res = np.array(res)\n",
    "    return(res)\n",
    "#\n",
    "\n",
    "def prep_dataset_aug(setd,nlcmaps,tlab=-1):\n",
    "    setd_f = []\n",
    "    setd_l = []\n",
    "    for i in setd:\n",
    "        arrimg = featureMap(i, nclmaps)\n",
    "        lbl = coils.loc[coils['SID']==i,'Label'].values[0]\n",
    "        if lbl == tlab: # if lower class => higher augmentation\n",
    "            res = factory_rep(arrimg,step=8)\n",
    "        else:\n",
    "            res = factory_rep(arrimg)\n",
    "        for j in range(res.shape[0]):\n",
    "            setd_f.append(res[j,:,:])\n",
    "            if lbl == 1:\n",
    "                setd_l.append([1.,0.])\n",
    "            if lbl == 2:\n",
    "                setd_l.append([0.,1.])\n",
    "    setd_f = np.array(setd_f)\n",
    "    setd_l = np.array(setd_l)\n",
    "    return([setd_f,setd_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64affff8-d837-4739-828a-b62b3d3158f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = 'C:/Users/alexm/OneDrive/Escritorio/UNIVERSIDAD/TFG/ALVAREZ_CASTRO_Alejandro/ALVAREZ_CASTRO_Alejandro/ZN_1D_imgs/'\n",
    "# for i in train + valid + test:\n",
    "train_f, train_l = prep_dataset_aug(train,nclmaps,2)\n",
    "np.savez(goal+'train.npz', features=train_f, labels=train_l)\n",
    "#\n",
    "valid_f, valid_l = prep_dataset(valid,nclmaps)\n",
    "np.savez(goal+'validation.npz', features=valid_f, labels=valid_l)\n",
    "#\n",
    "test_f, test_l = prep_dataset(test,nclmaps)\n",
    "np.savez(goal+'test.npz', features=test_f, labels=test_l)\n",
    "#\n",
    "ass_f, ass_l = prep_dataset(ass,nclmaps)\n",
    "np.savez(goal+'assess.npz', features=ass_f, labels=ass_l)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b67e1-3a10-426a-8c8b-bac30e6340f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "npzfile = np.load(goal+'train.npz')\n",
    "print(npzfile['features'].shape)\n",
    "print(npzfile['labels'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ec863",
   "metadata": {},
   "source": [
    "# 4. Building and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read a single configuration from a single file\n",
    "\n",
    "# # Load parameters from YAML file\n",
    "# with open('config.yaml') as file:\n",
    "#     configurations = yaml.safe_load(file)\n",
    "\n",
    "# # Get neural network parameters\n",
    "# network_params = config['network_params']\n",
    "\n",
    "# # Create an instance of ConvNetBuilder with loaded parameters\n",
    "# builder = ConvNetBuilder(**network_params)\n",
    "\n",
    "# # Build the model\n",
    "# model, name = builder.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read multiple configurations from a single file\n",
    "\n",
    "# with open('config.yaml') as file:\n",
    "#     configurations = yaml.safe_load(file)\n",
    "\n",
    "# # Iterate over configurations and build and train models\n",
    "# for config_name, config_params in configurations['configurations'].items():\n",
    "#     print(f\"Building and training model: {config_params['name']}\")\n",
    "    \n",
    "#     # Build model with current configuration parameters\n",
    "#     builder = ConvNetBuilder(**config_params)\n",
    "#     model, name = builder.build_model()\n",
    "    \n",
    "#     # Train the model and get metrics\n",
    "#     num_simulations = 1\n",
    "#     # metrics_plot, conf_plot, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1, mean_accuracy, std_dev_accuracy = simulations(name, num_simulations, model, train_f, train_l, valid_f, valid_l, ass_f, ass_l)\n",
    "#     metrics_plot, conf_plot = simulations(name, num_simulations, model, train_f, train_l, valid_f, valid_l, ass_f, ass_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read multiple configurations from multiple files\n",
    "\n",
    "directory = os.getcwd()\n",
    "yaml_files = [file for file in os.listdir(directory) if file.endswith('.yaml')]\n",
    "\n",
    "# Iterate over YAML files\n",
    "for yaml_file in yaml_files:\n",
    "    with open(os.path.join(directory, yaml_file)) as file:\n",
    "        configurations = yaml.safe_load(file)\n",
    "    \n",
    "    # Iterate over configurations in current YAML file\n",
    "    for config_name, config_params in configurations['configurations'].items():\n",
    "        print(f\"Building and training model: {config_params['name']} from file {yaml_file}\")\n",
    "        \n",
    "        # Build model with current configuration parameters\n",
    "        builder = ConvNetBuilder(**config_params)\n",
    "        model, name = builder.build_model()\n",
    "        \n",
    "        # Train the model and get metrics\n",
    "        num_simulations = 10\n",
    "        metrics_plot, conf_plot = simulations(yaml_file, name, num_simulations, model, train_f, train_l, valid_f, valid_l, ass_f, ass_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae6b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_simulations = 1\n",
    "\n",
    "# metrics_plot, conf_plot, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1, mean_accuracy, std_dev_accuracy = simulations(name, num_simulations, model, train_f, train_l, valid_f, valid_l, ass_f, ass_l)\n",
    "# metrics_plot, conf_plot = simulations(name, num_simulations, model, train_f, train_l, valid_f, valid_l, ass_f, ass_l)\n",
    "\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
